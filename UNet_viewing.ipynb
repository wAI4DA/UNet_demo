{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d80a21-cdd3-4a48-858e-58264e91f9c7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from FunctionsAndClasses.HEADER_FunctionsAndClasses import *\n",
    "from FunctionsAndClasses.HEADER_torch import *\n",
    "from FunctionsAndClasses.HEADER_utilities import *\n",
    "from FunctionsAndClasses.HEADER_plotting import *\n",
    "from FunctionsAndClasses.HEADER_models import *\n",
    "\n",
    "C = CONSTANTS()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e733dba-c11b-4130-9079-52a9f620e098",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARG_VAR=\"t2m\" #This example only deals with t2m, so leave as is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0116a32-696a-426d-98c2-91ef20bc2835",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_attrs = DefineModelAttributes(is_train=False)\n",
    "filename = f\"BS24_NE50_tD_pred(t2m)_targ(t2m)\" #model filename, as saved in Trained_models. Should omit \".pt\" at the end\n",
    "\n",
    "model_attrs.set_model_weights(filename) #This handles creation of the dataset, model savename, and loading model architecture + pretrained weights. If you want to view a model you trained, just change \"filename\" to your model's name!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de135fc4-f56a-4321-b1fb-56b820d5edf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "IDX = 0 #Change to view different times. Range = 0 to 8783 (hourly data for 2024)\n",
    "\n",
    "pred, targ, model_output, dt_current = get_model_output_at_idx(model_attrs=model_attrs, \n",
    "                                                                 model=model_attrs.model, \n",
    "                                                                 idx=IDX, \n",
    "                                                                 pred_var=TARG_VAR, \n",
    "                                                                 targ_var=TARG_VAR,\n",
    "                                                                 is_nan=True,\n",
    "                                                                 nan_fill_value=0)\n",
    "\n",
    "\n",
    "plot_predictor_output_truth_error_CONUS(pred, model_output, targ,\n",
    "                                        include_predictor=True,\n",
    "                                        include_model_output=True,\n",
    "                                        include_target=True,\n",
    "                                        date_str = dt_current,\n",
    "                                        title = f\"{model_attrs.savename}\",\n",
    "                                        error_units = f\"{C.varname_units_dict[TARG_VAR]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5997ebc-30fd-4417-9b29-db1b5553ab73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare the model output to Smartinit at the same date and time\n",
    "\n",
    "xr_smartinit = get_smartinit_output_at_idx(i=IDX, target_var=TARG_VAR)\n",
    "\n",
    "plot_predictor_output_truth_error_CONUS(pred, xr_smartinit.data, targ,\n",
    "                                        include_predictor=True,\n",
    "                                        include_model_output=True,\n",
    "                                        include_target=True,\n",
    "                                        date_str = dt_current,\n",
    "                                        title = f\"Smartinit\",\n",
    "                                        error_units = f\"{C.varname_units_dict[TARG_VAR]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6816d894-5139-49ae-ad72-3eb1648ae3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View a relative comparison of the two models' performance\n",
    "# The example UNet, BS24_NE50_tD_pred(t2m)_targ(t2m), was not trained over CONUS, and so it is generally worse than Smartinit, except in the west\n",
    "plot_model_vs_model_error(model_output, xr_smartinit.data, pred, targ, date_str=dt_current, error_units=f\",{C.varname_units_dict[TARG_VAR]} (+ = Smartinit is better)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11190db5-92f6-483c-8816-95de851b16da",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot a visualization of the training loss by using the training_log file \n",
    "## Commented by default - uncomment the 2 lines below and run this cell if you have a training log to visualize\n",
    "\n",
    "# training_log_filename = f\"training_log_BS{model_attrs.BATCH_SIZE}_NE{model_attrs.NUM_EPOCHS}.txt\" #Using the default notation from model_training.py - change this if your filename is different\n",
    "# plot_training_loss(training_log_filename, title_str=f\"{model_attrs.savename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
